---
title: "Session 4: Homework 2"
author: "Group 11: Vani Duggal, Mehak Khanna, Manon Pillot, Nick Chen, Liyang Zhang, Tobias Delago"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
editor_options: 
  markdown: 
    wrap: 72
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(wbstats)
library(countrycode)
library(patchwork)
library(gganimate)
library(infer)
library(lubridate)
library(knitr)
```

# Climate change and temperature anomalies

```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")
```


## Select the year and the twelve month variables from the `weather` dataset and convert the dataframe from wide to 'long' format. 

```{r tidyweather}
weather_selected <- weather %>% 
  select(1:13)

tidyweather <- weather_selected %>% 
  pivot_longer(cols = 2:13,
               names_to = "month",
               values_to = "delta") 

tidyweather

```

## Let us plot the data using a time-series scatter plot, and add a trendline. 

```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), month, "1")),
         month = month(date, label=TRUE))
         #Year = year(date))

tidyweather

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point(alpha = 0.4)+
  geom_smooth(color="red", se = FALSE) +
  theme_bw() +
  labs (
    title = "Weather Anomalies - Deviations (Base 1951-1980)",
    x = "Year",
    y = "Delta Temperature",
    caption = "Source: National Aeronautics and Space Administration - Goddard Institute (2022)"
  )

```

## Is the effect of increasing temperature more pronounced in some months?  

```{r facet_wrap, echo=FALSE}

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point(alpha = 0.4)+
  geom_smooth(color="red", se = FALSE) +
  facet_wrap(~month) +
  theme_bw() +
  labs (
    title = "Weather Anomalies - Deviations by Month (Base 1951-1980)",
    x = "Year",
    y = "Delta Temperature",
    caption = "Source: National Aeronautics and Space Administration - Goddard Institute (2022)"
  )


```
**Answer:**\

It looks like the colder months (Oct to April) have increased more significantly (steeper slope) than the summer months. This can also be confirmed by a report from the US Environmental Protection Agency which highlights that overall, minimum temperatures have increased at a higher rate than average maximum temperatures.



We remove data before 1800 and before using `filter`. Then, we use the `mutate` function to create a new variable `interval` which contains information on which period each observation belongs to. We can assign the different periods using `case_when()`.

```{r intervals, eval=FALSE}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))


```


## Create a density plot to study the distribution of monthly deviations (`delta`), grouped by the different time periods we are interested in. 

```{r density_plot, eval=FALSE}

ggplot(comparison, aes(x = delta, fill = interval)) +
  geom_density(alpha = 0.5)+
  labs (
    title = "Weather Anomalies - Deviations by Period (Base 1951-1980)",
    x = "Delta Temperature",
    y = "Density",
    caption = "Source: National Aeronautics and Space Administration - Goddard Institute (2022)"
  ) +
  theme_bw()

```

## Create a scatter plot for average annual anomalies. 

```{r averaging, eval=FALSE}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(meanDelta = mean(delta, na.rm=TRUE)) 

#plotting the data:

ggplot(average_annual_anomaly, aes(x = Year, y = meanDelta)) +
  geom_point()+
  #Fit the best fit line, using LOESS method
  geom_smooth(method="loess", colour="red")+
  labs (
    title = "Weather Anomalies - Deviations annual mean (Base 1951-1980)",
    x = "Year",
    y = "Delta Temperature",
    caption = "Source: National Aeronautics and Space Administration - Goddard Institute (2022)"
  ) +
  #change theme to theme_bw() to have white background + black frame around plot
  theme_bw()

```

## Confidence Interval for `delta`

```{r, calculate_CI_using_formula}

formula_ci <- comparison %>% 
  
  # choose the interval 2011-present
  filter(interval == "2011-present") %>% 
  group_by(interval) %>% 
  
  # calculate summary statistics for temperature deviation (delta) 
  # calculate mean, SD, count, SE, lower/upper 95% CI
  summarize(mean = mean(delta, na.rm=TRUE),
            sd = sd(delta, na.rm=TRUE),
            count = n(),
            t_critical = qt(0.975, count-1),
            se_delta = sd/sqrt(count),
            margin_of_error = t_critical * se_delta,
            delta_low = mean - margin_of_error,
            delta_high = mean + margin_of_error)
#print out CI
kable(formula_ci)

```

```{r}
# bootstrap simulation from here

set.seed(1234)

boot_delta <- comparison %>% 
  filter(interval == "2011-present") %>% 
  specify(response = delta) %>% 
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat="mean")

bootstrap_ci <- boot_delta %>% 
  get_confidence_interval(level =0.95, type = "percentile")

#print out CI
kable(bootstrap_ci)
```


**Answer:**\

We started by calculating the lower and upper boundary with the formula. This is done by using the t-distribution available in R (qt) and multiplying it with the standard error. The bootstrap simulation on the other hand creates 1000 repetitions out of the given data (delta in this case) and uses the get_confidence_interval function from the infer package to create the delta_low and delta_high value. First of all it can be said that with 1000 iterations, the bootstrap simulation yields the exact same results as the formula. Further the count (144) allows to have a low t_critical as well as a small standard error, producing a quite narrow confidence interval. 


# Biden's Approval Margins


```{r, cache=TRUE}
# Import approval polls data directly off fivethirtyeight website
approval_polllist <- read_csv('https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv') 

```

## Lubridate the chr dates

```{r}
# Use `lubridate` to fix dates, as they are given as characters.

approval_date <- approval_polllist %>% 
  mutate(enddate = mdy(enddate))

```


## Create a plot for net approval rating for each week in 2022

Your plot should look something like this:

```{r trump_margins, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "biden_approval_margin.png"), error = FALSE)
```


```{r, fig.height=4}
poll_filtered <- approval_date %>% 
  select(c("enddate","subgroup","approve","disapprove")) %>% 
  filter(enddate >= "2022-01-01") %>% 
  mutate(week = week(enddate),
         dif = approve-disapprove)

ci_biden_facet <- poll_filtered %>% 
  group_by(week, subgroup) %>% 
  summarize(mean = mean(dif, na.rm=TRUE),
            sd = sd(dif, na.rm=TRUE),
            count = n(),
            t_critical = qt(0.975, count-1),
            se_diff = sd/sqrt(count),
            margin_of_error = t_critical * se_diff,
            diff_low = mean - margin_of_error,
            diff_high = mean + margin_of_error)

ggplot(ci_biden_facet, aes(x = week, y = mean, color = subgroup)) +
  geom_line() +
  geom_ribbon(aes(x=week, y=mean, ymax=diff_high, ymin=diff_low, color = subgroup), linetype=1, size = 0.9, fill = "brown", alpha=0.1)+
  facet_wrap(~subgroup, ncol = 1, strip.position="right") +
  labs(
    title = "Biden's net Approval Ratings in 2022",
    subtitle = "weekly data, approval - disapproval, %",
    y = NULL, 
    x = "Week in 2022",
    caption = "Source: https://projects.fivethirtyeight.com/Biden-approval-data/"
  ) +
  theme_bw() +
  theme(legend.position="none")


```

# Challenge 1: Excess rentals in TfL bike sharing

Recall the TfL data on how many bikes were hired every single day. We
can get the latest data by running the following

```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```

We can easily create a facet grid that plots bikes hired by month and
year since 2015

```{r tfl_month_year_grid, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

However, the challenge I want you to work on is to reproduce the
following two graphs.

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```

The second one looks at percentage changes from the expected level of
weekly rentals. The two grey shaded rectangles correspond to Q2 (weeks
14-26) and Q4 (weeks 40-52).

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```

For both of these graphs, you have to calculate the expected number of
rentals per week or month between 2016-2019 and then, see how each
week/month of 2020-2022 compares to the expected rentals. Think of the
calculation `excess_rentals = actual_rentals - expected_rentals`.

Should you use the mean or the median to calculate your expected
rentals? Why?

In creating your plots, you may find these links useful:

-   <https://ggplot2.tidyverse.org/reference/geom_ribbon.html>
-   <https://ggplot2.tidyverse.org/reference/geom_tile.html>
-   <https://ggplot2.tidyverse.org/reference/geom_rug.html>

# Challenge 2: Share of renewable energy production in the world

The National Bureau of Economic Research (NBER) has a a very interesting
dataset on the adoption of about 200 technologies in more than 150
countries since 1800. This is the[Cross-country Historical Adoption of
Technology (CHAT) dataset](https://www.nber.org/research/data/cross-country-historical-adoption-technology).

The following is a description of the variables

| **variable** | **class** | **description**                |
|--------------|-----------|--------------------------------|
| variable     | character | Variable name                  |
| label        | character | Label for variable             |
| iso3c        | character | Country code                   |
| year         | double    | Year                           |
| group        | character | Group (consumption/production) |
| category     | character | Category                       |
| value        | double    | Value (related to label)       |

```{r,load_technology_data}

technology <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-19/technology.csv')

#get all technologies
labels <- technology %>% 
  distinct(variable, label)

# Get country names using 'countrycode' package
technology <- technology %>% 
  filter(iso3c != "XCD") %>% 
  mutate(iso3c = recode(iso3c, "ROM" = "ROU"),
         country = countrycode(iso3c, origin = "iso3c", destination = "country.name"),
         country = case_when(
           iso3c == "ANT" ~ "Netherlands Antilles",
           iso3c == "CSK" ~ "Czechoslovakia",
           iso3c == "XKX" ~ "Kosovo",
           TRUE           ~ country))

#make smaller dataframe on energy
energy <- technology %>% 
  filter(category == "Energy")

# download CO2 per capita from World Bank using {wbstats} package
# https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1970, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated))

# get a list of countries and their characteristics
# we just want to get the region a country is in and its income level
countries <-  wb_cachelist$countries %>% 
  select(iso3c,region,income_level)

```

This is a very rich data set, not just for energy and CO2 data, but for many other technologies. In our case, we just need to produce a couple of graphs-- at this stage, the emphasis is on data manipulation, rather than making the graphs gorgeous.

First, produce a graph with the countries with the highest and lowest % contribution of renewables in energy production. This is made up of `elec_hydro`, `elec_solar`, `elec_wind`, and `elec_renew_other`. You may want to use the *patchwork* package to assemble the two charts next to each other.
 
```{r min-max_renewables, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "renewables.png"), error = FALSE)
```
Second, you can produce an animation to explore the relationship between CO2 per capita emissions and the deployment of renewables. As the % of energy generated by renewables goes up, do CO2 per capita emissions seem to go down?

 
```{r animation, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "animation.gif"), error = FALSE)
```

To create this animation is actually straight-forward. You manipulate your date, and the create the graph in the normal ggplot way. the only `gganimate` layers you need to add to your graphs are

```
  labs(title = 'Year: {frame_time}', 
       x = '% renewables', 
       y = 'CO2 per cap') +
  transition_time(year) +
  ease_aes('linear')
```


# Deliverables

As usual, there is a lot of explanatory text, comments, etc. You do not
need these, so delete them and produce a stand-alone document that you
could share with someone. Knit the edited and completed R Markdown file
as an HTML document (use the "Knit" button at the top of the script
editor window) and upload it to Canvas.

# Details

-   Who did you collaborate with: TYPE NAMES HERE
-   Approximately how much time did you spend on this problem set:
    ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute
rule](https://mam202.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}.
You know enough R (and have enough examples of code from class and your
readings) to be able to do this. If you get stuck, ask for help from
others, post a question on Slack-- and remember that I am here to help
too!

> As a true test to yourself, do you understand the code you submitted
> and are you able to explain it to someone else?

# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all
components. Code is poorly written and not documented. Uses the same
type of plot for each graph, or doesn't use plots appropriate for the
variables being analyzed.

Check (3/5): Solid effort. Hits all the elements. No clear mistakes.
Easy to follow (both the code and the output).

Check plus (5/5): Finished all components of the assignment correctly
and addressed both challenges. Code is well-documented (both
self-documented and with additional comments as necessary). Used
tidyverse, instead of base R. Graphs and tables are properly labelled.
Analysis is clear and easy to follow, either because graphs are labeled
clearly or you've written additional text to describe how you interpret
the output.
